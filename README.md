# ðŸ§  IndustryForecast: Machine Learningâ€“Based Industrial Workforce Prediction

This project analyzes changes in regional industrial structures using real-world employment data from 2017 to 2022.  
By combining statistical analysis and machine learning, it predicts the direction of industrial growth and decline for 2023.  

Chi-square tests were used to verify that industrial compositions differ significantly between population-increasing and population-decreasing regions.  
Subsequently, Random Forest and K-Nearest Neighbor classifiers were trained to model these regional trends, and a soft-voting ensemble achieved the highest accuracy.  

Results show that:
- **Population-decreasing regions** are dominated by **construction (26%)**, **health & social welfare (16%)**, and **manufacturing (12%)**.  
- **Population-increasing regions** are led by **manufacturing (18%)**, **professional & technical services (13%)**, and **retail trade (13%)**.  
- The model achieved an accuracy of **83%**, with strong generalization in predicting industrial growth patterns.  

These findings provide an interpretable foundation for **regional policy planning** and **industrial strategy design** based on workforce transitions.
--- 
## ðŸ§° Technologies Used
Python, scikit-learn, pandas, matplotlib, seaborn, NumPy
---
## ðŸ“‚ Project Structure
```
â”œâ”€â”€ main.py # Training and validation logic for RNN & LSTM models
â”œâ”€â”€ generate.py # Generates text using the trained model
â”œâ”€â”€ model.py # Defines Vanilla RNN & LSTM architectures
â”œâ”€â”€ dataset.py # Loads and preprocesses the Shakespeare dataset
â””â”€â”€ shakespeare_train.txt # Source text data for training
```
## ðŸš€ How to Run
### Clone the repository
```bash
git clone https://github.com/daewook1004/DeepLearning_LangugeModel.git
cd DeepLearning_LangugeModel
```
### Install dependencies
```bash
pip install torch numpy tqdm
```
### Train the models
Run the training script to train both RNN and LSTM models:
```bash
python main.py
```
The script will automatically train both models, log training/validation losses,
and save the best models as best_rnn_model.pth and best_lstm_model.pth.

### Generate Shakespeare-style text
Once training is complete, generate new text using the trained LSTM model:
```bash
python generate.py
```
You can modify the seed character and temperature in generate.py
to control creativity and text variation.

### View results
Generated samples and loss plots are saved in:
```bash
graph_pictures/
best_lstm_model.pth
best_rnn_model.pth
```

---
## ðŸ“‰ RNN Training & Validation Loss
<div align="center">
    <img src="https://github.com/daewook1004/DeepLearning_LangugeModel/assets/113410302/d0f9ee37-4fdd-4ab2-9d1b-9a5b9a62b5e4" width="45%">
    <img src="https://github.com/daewook1004/DeepLearning_LangugeModel/assets/113410302/58bca3e4-3086-44db-962d-21008860b191" width="45%">
</div>
Both training and validation loss steadily decreased across 20 epochs.

---
## ðŸ“‰ LSTM Training & Validation Loss
<div align="center">
    <img src="https://github.com/daewook1004/DeepLearning_LangugeModel/assets/113410302/b8011d8c-0520-4709-ba92-473a4db44605" width="45%">
    <img src="https://github.com/daewook1004/DeepLearning_LangugeModel/assets/113410302/0b6d6681-91bf-4978-91a0-20e463976c73" width="45%">
</div>

âœ… The LSTM model showed smoother convergence and lower overall loss compared to RNN.
## âš™ï¸ Training Parameters

| Parameter        | Value  |
|------------------|--------|
| sequence_length  | 30     |
| batch_size       | 64     |
| n_layers         | 2      |
| hidden_size      | 128    |
| dropout          | 0.3    |
| num_epochs       | 20     |
| learning_rate    | 0.0001 |

---

## ðŸ“Š Model Performance Comparison

| Epoch | Model | Training Loss | Validation Loss |
|:------:|:------:|:---------------:|:----------------:|
| 1 | RNN  | 2.2198 | 1.9893 |
| 1 | LSTM | 2.3715 | 2.0325 |
| 20 | RNN  | 1.5366 | 1.6502 |
| 20 | LSTM | 1.3779 | 1.5960 |

> ðŸ”¹ **LSTM outperformed RNN**, achieving lower training and validation loss and demonstrating stronger long-term dependency learning.

---

## âœï¸ Text Generation (Best Model: LSTM)

### ðŸ”§ Parameters
- **Seed Characters:** `['F', 'K', 'N', 'S', 'U']`
- **Temperatures:** `[0.3, 0.5, 0.7, 1.0, 1.5]` (No generation below 0.3)

---
### ðŸ§¾ Example Outputs

#### Temperature = 0.3
> Grammatically consistent but low creativity.
<h5>Seed Character: F  --------------     First Senator: What they have been the forth of the people, That we will not be the country to the pe</h5> 
<h5>Seed Character: K  --------------     KINGHAM: How now the senate, and the people. First Senator: So, the gods be many dear with him.</h5>
<h5>Seed Character: N  --------------     NENIUS: We have been the gods shall be somether send the people, and his country's son, That have not</h5>
<h5>Seed Character: S  --------------     S: The common her with a man of the world be so dear means and his father should be thee.</h5>
<h5>Seed Character: U  --------------     US: I were your heart, that which they are a senators, I will not be the people.</h5>

#### Temperature = 0.7
> Balanced between structure and creativity; closely resembles Shakespeareâ€™s tone.

<h5>Seed Character: F  --------------     From me but to power we have too the people; and excoes me? SICINIUS: Go he lay your long.</h5> 
<h5>Seed Character: K  --------------     KINGHAM: Ke more for the people do you do you done: The noble companion that cry show the first buttervingman;</h5>
<h5>Seed Character: N  --------------     NIUS: I will and before them for they are To great bears and his whole hearts, where in been by for t</h5>
<h5>Seed Character: S  --------------     S: A will be speak to do not the happy alone. SICINIUS: You have remain my true speak the dather.</h5>
<h5>Seed Character: U  --------------     US: Do not to back of these state, I have been one to ears, my part of mercy, By such that you made t</h5>

#### Temperature = 1.0 
> Highly creative but grammatically inconsistent.

<h5>Seed Character: F  --------------     First Lords: Fie, done you thy country. BRUTUS: The name of thy such fonsolets: What makes me but am</h5> 
<h5>Seed Character: K  --------------     Kiend all me, sir, which at your heart Than parsing the rock that the ragh, And two be virtue the dic</h5>
<h5>Seed Character: N  --------------     NIUS: It deservous suelingment are Hath briefst I have but Corioli me that you have pardon: Standing,</h5>
<h5>Seed Character: S  --------------     S: Our award. Second Servingman: You say serve comes death with shown myself? And spir of yourselves </h5>
<h5>Seed Character: U  --------------     UFIDIUS: Should yet you As I can less my bond my chance Frame? our country's end Bratthe thine this, </h5>

#### Temperature = 1.5
<h5>Seed Character: F  --------------     F'd of. Which anstand you? You, grannevew bodeved! Towarls: and Tewly Put thy dix, getchilargunate, o</h5> 
<h5>Seed Character: K  --------------     Kt I, to to go.; dediment. Sull us. LACTIUS: Propeht, to be now? MENENIUS: I I'll stative Whyerfest </h5>
<h5>Seed Character: N  --------------     Nd, tew waoldren make my isomplanown Co i' their knowy in do in You; if;, Besterly, may, had's royhly</h5>
<h5>Seed Character: S  --------------     Second Citizen: Ye, Tower, you quayst King. Delives? Colome, From these scunce-falder blood MA'Rire:</h5>
<h5>Seed Character: U  --------------     US: Thou! looktare myou, Motaltewive, sirs-nris'lls, writy.n Like noble madam Wife methools led fool </h5>

# ðŸ’¬ Discussion
- **Low Temperature (0.3â€“0.5):** High grammatical accuracy, low creativity.  
- **High Temperature (1.0â€“1.5):** High creativity, frequent grammatical errors.  
- **Optimal Range (0.7â€“1.0):** Most natural and expressive Shakespearean-style text generation.

> The LSTM model with temperature values between **0.7â€“1.0** best reproduces the poetic and rhythmic qualities of Shakespeareâ€™s writing.
